{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bac12d5-191a-4322-a0ba-e7cf5c004f40",
   "metadata": {},
   "source": [
    "# Evaluating BART on Training and Testing Data.\n",
    "\n",
    "### This notebook contains a class with functions designed to evaluate BART's accuracy on my specific Reddit label set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1a006a5-b96d-494d-8dc8-14fce6503276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import torch\n",
    "from transformers import BartForSequenceClassification, BartTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5298a6c-8e01-40d9-99cb-db5119620a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class eval_BART():\n",
    "\n",
    "    # -------------------------------------------------------------------------------------\n",
    "    # This class configures data and uses BART's zero shot model to evaluate BART's accuracy on my sentiment label set.\n",
    "    # -------------------------------------------------------------------------------------\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        # initialize class with tokenizer and model.\n",
    "        # -------------------------------------------------------------------------------------\n",
    "\n",
    "        model_name = 'facebook/bart-large-mnli'\n",
    "        self.tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "        self.model = BartForSequenceClassification.from_pretrained(model_name, num_labels=3, multi_label=True)\n",
    "        \n",
    "        return \n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    def bin_conf_scores_via_premise(self, data):\n",
    "        \n",
    "        # -------------------------------------------------------------------------------------\n",
    "        # Given a pandas dataframe, evaluate each row using BART. A confidence score will be assigned \n",
    "        # to each premise. At the end, we bin each premise under their respective confidence score.\n",
    "        #\n",
    "        # Returns a dictionary where key = 'confidence score interval', and value = 'premise'.\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        LABELS = [\"contradiction\", \"neutral\", \"entailment\"]\n",
    "        high_conf_list = []\n",
    "\n",
    "        # Key = confidence interval ... value = premise (reddit text)\n",
    "        high_conf_dict = {\n",
    "            \"97-100\": [],\n",
    "            \"95-97\": [],\n",
    "            \"90-95\": [],\n",
    "            \"80-90\": [],\n",
    "            \"60-80\": [],\n",
    "            \"40-60\": [],\n",
    "            \"0-40\": []\n",
    "        }\n",
    "\n",
    "        # Iterate through our pandas data to get the current row information.\n",
    "        for _, row in data.iterrows():\n",
    "            premise = row[\"premise\"]\n",
    "            hypothesis = row[\"hypothesis\"]\n",
    "\n",
    "            # Get Barts confidence score, given the premise and hypothesis.\n",
    "            inputs = self.tokenizer(\n",
    "                premise,\n",
    "                hypothesis,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=True\n",
    "            )\n",
    "            scores = torch.softmax(self.model(**inputs).logits, dim=-1)\n",
    "            scores = scores.squeeze().tolist()\n",
    "\n",
    "            # Throw the premise into its respective slot.\n",
    "            if scores[2] > .97:\n",
    "                high_conf_dict[\"97-100\"].append(premise)\n",
    "            elif scores[2] >= .95 and scores[2] < .97:\n",
    "                high_conf_dict[\"95-97\"].append(premise)\n",
    "            elif scores[2] >= .90 and scores[2] < .95:\n",
    "                high_conf_dict[\"90-95\"].append(premise)\n",
    "            elif scores[2] >= .80 and scores[2] < .90:\n",
    "                high_conf_dict[\"80-90\"].append(premise)\n",
    "            elif scores[2] >= .60 and scores[2] < .80:\n",
    "                high_conf_dict[\"60-80\"].append(premise)\n",
    "            elif scores[2] >= .40 and scores[2] < .60:\n",
    "                high_conf_dict[\"40-60\"].append(premise)\n",
    "            elif scores[2] < .40:\n",
    "                high_conf_dict[\"0-40\"].append(premise)\n",
    "\n",
    "        # Returns a dictionary where key = 'confidence score interval', and value = 'premise'.\n",
    "        return high_conf_dict\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def value_to_num(self, score_dict):\n",
    "\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        # Given a dictionary from the bin_conf_scores_via_premise function, turn those values into\n",
    "        # numbers rather than text examples. The numbers represent the amount of text files in each \n",
    "        # category.\n",
    "        #\n",
    "        # Returns a dictionary where key = 'confidence score interval', and value = 'count of premise'\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        for key in score_dict.keys():\n",
    "            new_val = len(score_dict[key])\n",
    "            score_dict[key] = new_val\n",
    "\n",
    "        # Returns a dictionary where key = 'confidence score interval', and value = 'count of premise'\n",
    "        return score_dict\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def bin_conf_scores_via_premise_count(self, data):\n",
    "\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        # Given a pandas dataframe, evaluate each row using BART. A confidence score will be assigned \n",
    "        # to each premise. At the end, we bin the sum of hypothesis types for each confidence interval.\n",
    "        #\n",
    "        # Returns a nested dictionary where root_key = 'confidence score interval', key = 'hypothesis phrase', value = count.\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        LABELS = [\"contradiction\", \"neutral\", \"entailment\"]\n",
    "        high_conf_list = []\n",
    "\n",
    "        # Nested dictionary where confidence interval points to hypothesis phrases and their counts.\n",
    "        high_conf_dict = {\n",
    "            \"97-100\": {\"This text is about politics.\": 0,\n",
    "                       \"This text expresses gratitude.\": 0,\n",
    "                       \"This text expresses frustration.\": 0,\n",
    "                       \"This text is focused on solutions.\": 0,\n",
    "                       \"This text provides information or news.\": 0,\n",
    "                       \"This text expresses fear or panic.\": 0,\n",
    "                       \"This text contains blaming.\": 0,\n",
    "                       \"This text is seeking help or advice.\": 0,\n",
    "                       \"This text is about wildfires.\": 0,\n",
    "                       \"This text is about prescribed burns.\": 0,\n",
    "                       \"This text is about fire management.\": 0},\n",
    "            \"95-97\": {\"This text is about politics.\": 0,\n",
    "                       \"This text expresses gratitude.\": 0,\n",
    "                       \"This text expresses frustration.\": 0,\n",
    "                       \"This text is focused on solutions.\": 0,\n",
    "                       \"This text provides information or news.\": 0,\n",
    "                       \"This text expresses fear or panic.\": 0,\n",
    "                       \"This text contains blaming.\": 0,\n",
    "                       \"This text is seeking help or advice.\": 0,\n",
    "                       \"This text is about wildfires.\": 0,\n",
    "                        \"This text is about prescribed burns.\": 0,\n",
    "                       \"This text is about fire management.\": 0},\n",
    "            \"90-95\": {\"This text is about politics.\": 0,\n",
    "                       \"This text expresses gratitude.\": 0,\n",
    "                       \"This text expresses frustration.\": 0,\n",
    "                       \"This text is focused on solutions.\": 0,\n",
    "                       \"This text provides information or news.\": 0,\n",
    "                       \"This text expresses fear or panic.\": 0,\n",
    "                       \"This text contains blaming.\": 0,\n",
    "                       \"This text is seeking help or advice.\": 0,\n",
    "                       \"This text is about wildfires.\": 0,\n",
    "                       \"This text is about prescribed burns.\": 0,\n",
    "                       \"This text is about fire management.\": 0},\n",
    "            \"80-90\": {\"This text is about politics.\": 0,\n",
    "                       \"This text expresses gratitude.\": 0,\n",
    "                       \"This text expresses frustration.\": 0,\n",
    "                       \"This text is focused on solutions.\": 0,\n",
    "                       \"This text provides information or news.\": 0,\n",
    "                       \"This text expresses fear or panic.\": 0,\n",
    "                       \"This text contains blaming.\": 0,\n",
    "                       \"This text is seeking help or advice.\": 0,\n",
    "                       \"This text is about wildfires.\": 0,\n",
    "                       \"This text is about prescribed burns.\": 0,\n",
    "                       \"This text is about fire management.\": 0},\n",
    "            \"60-80\": {\"This text is about politics.\": 0,\n",
    "                       \"This text expresses gratitude.\": 0,\n",
    "                       \"This text expresses frustration.\": 0,\n",
    "                       \"This text is focused on solutions.\": 0,\n",
    "                       \"This text provides information or news.\": 0,\n",
    "                       \"This text expresses fear or panic.\": 0,\n",
    "                       \"This text contains blaming.\": 0,\n",
    "                       \"This text is seeking help or advice.\": 0,\n",
    "                       \"This text is about wildfires.\": 0,\n",
    "                       \"This text is about prescribed burns.\": 0,\n",
    "                       \"This text is about fire management.\": 0},\n",
    "            \"40-60\": {\"This text is about politics.\": 0,\n",
    "                       \"This text expresses gratitude.\": 0,\n",
    "                       \"This text expresses frustration.\": 0,\n",
    "                       \"This text is focused on solutions.\": 0,\n",
    "                       \"This text provides information or news.\": 0,\n",
    "                       \"This text expresses fear or panic.\": 0,\n",
    "                       \"This text contains blaming.\": 0,\n",
    "                        \"This text is seeking help or advice.\": 0,\n",
    "                       \"This text is about wildfires.\": 0,\n",
    "                        \"This text is about prescribed burns.\": 0,\n",
    "                       \"This text is about fire management.\": 0},\n",
    "            \"0-40\": {\"This text is about politics.\": 0,\n",
    "                       \"This text expresses gratitude.\": 0,\n",
    "                       \"This text expresses frustration.\": 0,\n",
    "                       \"This text is focused on solutions.\": 0,\n",
    "                       \"This text provides information or news.\": 0,\n",
    "                       \"This text expresses fear or panic.\": 0,\n",
    "                       \"This text contains blaming.\": 0,\n",
    "                       \"This text is seeking help or advice.\": 0,\n",
    "                       \"This text is about wildfires.\": 0,\n",
    "                       \"This text is about prescribed burns.\": 0,\n",
    "                       \"This text is about fire management.\": 0},\n",
    "        }\n",
    "\n",
    "        # Iterate through our pandas data to get the current row information.\n",
    "        for _, row in data.iterrows():\n",
    "            premise = row[\"premise\"]\n",
    "            hypothesis = row[\"hypothesis\"]\n",
    "            label = row[\"label\"]\n",
    "        \n",
    "            inputs = self.tokenizer(\n",
    "                premise,\n",
    "                hypothesis,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=True\n",
    "            )\n",
    "            scores = torch.softmax(self.model(**inputs).logits, dim=-1)\n",
    "            scores = scores.squeeze().tolist()\n",
    "\n",
    "            # Add a count value to the correct interval -> hypothesis phrasing.\n",
    "            if scores[2] > .97:\n",
    "                high_conf_dict[\"97-100\"][hypothesis] += 1\n",
    "            elif scores[2] >= .95 and scores[2] < .97:\n",
    "                high_conf_dict[\"95-97\"][hypothesis] += 1\n",
    "            elif scores[2] >= .90 and scores[2] < .95:\n",
    "                high_conf_dict[\"90-95\"][hypothesis] += 1\n",
    "            elif scores[2] >= .80 and scores[2] < .90:\n",
    "                high_conf_dict[\"80-90\"][hypothesis] += 1\n",
    "            elif scores[2] >= .60 and scores[2] < .80:\n",
    "                high_conf_dict[\"60-80\"][hypothesis] += 1\n",
    "            elif scores[2] >= .40 and scores[2] < .60:\n",
    "                high_conf_dict[\"40-60\"][hypothesis] += 1\n",
    "            elif scores[2] < .40:\n",
    "                high_conf_dict[\"0-40\"][hypothesis] += 1\n",
    "\n",
    "        # Returns a nested dictionary where root_key = 'confidence score interval', key = 'hypothesis phrase', value = count.\n",
    "        return high_conf_dict\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "    def eval_perc(self, data, thresh):\n",
    "\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        # Given a dict from the bin_conf_scores_via_premise_count function, convert those values into \n",
    "        # lists [hypothesis count past threshold, total hypothesis count], so that they can be turned into \n",
    "        # percentages.\n",
    "        #\n",
    "        # Returns a dictionary where key = 'hypothesis phrasing', and value = ratio list.\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        perc_dict = {\n",
    "          'This text is about politics.': [0,0],\n",
    "          'This text expresses gratitude.': [0,0],\n",
    "          'This text expresses frustration.': [0,0],\n",
    "          'This text is focused on solutions.': [0,0],\n",
    "          'This text provides information or news.': [0,0],\n",
    "          'This text expresses fear or panic.': [0,0],\n",
    "          'This text contains blaming.': [0,0],\n",
    "          'This text is seeking help or advice.': [0,0],\n",
    "          'This text is about wildfires.': [0,0],\n",
    "          'This text is about prescribed burns.': [0,0],\n",
    "          'This text is about fire management.': [0,0]\n",
    "        }\n",
    "        date_range_conv = {\n",
    "            \"97-100\": 97,\n",
    "            \"95-97\": 95,\n",
    "            \"90-95\": 90,\n",
    "            \"80-90\": 80,\n",
    "            \"60-80\": 60,\n",
    "            \"40-60\": 40,\n",
    "            \"0-40\": 40\n",
    "        }\n",
    "\n",
    "        # Iterate confidence interval -> hypothesis phrasing, and add counts to the correct list index.\n",
    "        for date_range in data.keys():\n",
    "            for text in data[date_range].keys():\n",
    "                if date_range_conv[date_range] >= thresh:\n",
    "                    perc_dict[text][0] += data[date_range][text]\n",
    "    \n",
    "                perc_dict[text][1] += data[date_range][text]\n",
    "\n",
    "        # Returns a dictionary where key = 'hypothesis phrasing', and value = ratio list.\n",
    "        return perc_dict\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "    def turn_into_perc(self, data_dict):\n",
    "\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        # Given a dict from the eval_perc function, convert those list values into percentages.\n",
    "        #\n",
    "        # Returns a dictionary where key = 'hypothesis phrasing', and value = percentage.\n",
    "        # -------------------------------------------------------------------------------------\n",
    "\n",
    "        perc_dict = {\n",
    "          'This text is about politics.': 0,\n",
    "          'This text expresses gratitude.': 0,\n",
    "          'This text expresses frustration.': 0,\n",
    "          'This text is focused on solutions.': 0,\n",
    "          'This text provides information or news.': 0,\n",
    "          'This text expresses fear or panic.': 0,\n",
    "          'This text contains blaming.': 0,\n",
    "          'This text is seeking help or advice.': 0,\n",
    "          'This text is about wildfires.': 0,\n",
    "          'This text is about prescribed burns.': 0,\n",
    "          'This text is about fire management.': 0\n",
    "        }\n",
    "\n",
    "        # Iterate through dictionary and devide list[1] by list[0] to get the percentage.\n",
    "        # If list[0] = 0, return 0 to avoid devision by 0.\n",
    "        for text in data_dict.keys():\n",
    "            if data_dict[text][0] == 0:\n",
    "                perc_dict[text] = 0\n",
    "            else:\n",
    "                perc_dict[text] = data_dict[text][0] / data_dict[text][1]\n",
    "\n",
    "        # Returns a dictionary where key = 'hypothesis phrasing', and value = percentage.\n",
    "        return perc_dict\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def change_hyp_phrase(self, data, old_hyp, new_hyp):\n",
    "\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        # Given a pandas dataframe, change a hypothesis phrasing \n",
    "        #\n",
    "        # Returns a new dataframe with the corrected phrasing.\n",
    "        # -------------------------------------------------------------------------------------\n",
    "\n",
    "        data.loc[data[\"hypothesis\"] == old_hyp, \"hypothesis\"] = new_hyp\n",
    "\n",
    "        # Returns a new dataframe with the corrected phrasing.\n",
    "        return data \n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def graph_score_distribution(self, len_score_dict, data_name=\"\"):\n",
    "\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        # Given a score dict and data name (for labeling purposes), plot the amount of text that appears under each interval.\n",
    "        #\n",
    "        # Returns nothing, graphs as it goes.\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        keys = list(len_score_dict.keys())\n",
    "        values = list(len_score_dict.values())\n",
    "    \n",
    "        plt.figure()\n",
    "        for i in range(len(keys)):\n",
    "            plt.bar(keys[i], values[i])\n",
    "    \n",
    "        plt.title(\"confidence scores from current bart model - \" + data_name)\n",
    "        plt.xlabel(\"confidence level\")\n",
    "        plt.ylabel(\"text count\")\n",
    "        plt.show()\n",
    "    \n",
    "        return \n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def create_word_clouds(self, score_text_list, data_name=\"\"):\n",
    "\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        # Given a text list from the iterate_word_cloud function, create a word cloud.\n",
    "        #\n",
    "        # Returns nothing, graphs as it goes.\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        fig_name = str(data_name + \".png\")\n",
    "        full_string_words = ' '.join(score_text_list)\n",
    "    \n",
    "        wordcloud = WordCloud(width=350,\n",
    "                              height=150,\n",
    "                              background_color='white',\n",
    "                              colormap=\"coolwarm\").generate(full_string_words)\n",
    "    \n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.title(data_name + \" - Word Cloud\")\n",
    "        plt.axis('off')\n",
    "        plt.savefig(fig_name, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "    \n",
    "        return \n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def iterate_word_cloud(self, score_dict):\n",
    "    \n",
    "        # -------------------------------------------------------------------------------------\n",
    "        # Given a score dict, iterate through each interval to pass through the create_word_clouds \n",
    "        # function. \n",
    "        #\n",
    "        # Returns nothing.\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        for key in score_dict.keys():\n",
    "            if len(score_dict[key]) > 0:\n",
    "                create_word_clouds(score_dict[key], data_name=key)\n",
    "    \n",
    "        return \n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def retrieve_text_via_conf(self, data, conf_score, rand=True, type=\"entailment\"):\n",
    "\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        # Given a text dict, print each premise in the requested 'confidence score interval' \n",
    "        #\n",
    "        # Returns nothing.\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        \n",
    "        print(type, \" - text for scores with confidence score = \", conf_score)\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "    \n",
    "        count = 0\n",
    "        for dict in range(len(data[conf_score][\"text\"])):\n",
    "            print(\"text: \", data[conf_score][\"text\"][count])\n",
    "            print(\"hypo: \", data[conf_score][\"hyp\"][count])\n",
    "            print()\n",
    "            count += 1\n",
    "    \n",
    "            if (count%len(data[conf_score][\"text\"])) == 0:\n",
    "                count = 0\n",
    "                print(\"-----------------------------------------------------------------\")\n",
    "        \n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83a7b959-ec5f-4442-b8aa-7b799f455ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joeycartwright/miniconda3/envs/csci477/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "BART_eval_class = eval_BART()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7e8d41-413e-41dc-8c7a-75560697bd3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
